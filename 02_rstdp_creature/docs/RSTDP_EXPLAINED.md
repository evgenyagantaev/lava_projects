# Reward-Modulated STDP (R-STDP): Теория и Практика

## Содержание

1. [Введение](#введение)
2. [Теоретические основы](#теоретические-основы)
   - [Классический STDP](#классический-stdp)
   - [Проблема отложенного вознаграждения](#проблема-отложенного-вознаграждения)
   - [R-STDP: Трёхфакторное правило обучения](#r-stdp-трёхфакторное-правило-обучения)
   - [Eligibility Traces (следы приемлемости)](#eligibility-traces-следы-приемлемости)
3. [Реализация в симуляции](#реализация-в-симуляции)
   - [Архитектура нейронной сети](#архитектура-нейронной-сети)
   - [Генерация Reward-сигнала](#генерация-reward-сигнала)
   - [Прямой проход и накопление Eligibility Traces](#прямой-проход-и-накопление-eligibility-traces)
   - [Модуляция синаптических весов](#модуляция-синаптических-весов)
   - [Дополнительные механизмы](#дополнительные-механизмы)
4. [Математическое описание](#математическое-описание)
5. [Заключение](#заключение)

---

## Введение

**Reward-Modulated Spike-Timing Dependent Plasticity (R-STDP)** — это биологически правдоподобное правило обучения для спайковых нейронных сетей (SNN), которое объединяет:

1. **STDP** — локальное хеббовское обучение на основе временных корреляций спайков
2. **Reward Signal** — глобальный сигнал подкрепления, модулирующий пластичность

Это позволяет сети обучаться методом проб и ошибок, аналогично обучению с подкреплением (Reinforcement Learning), но с использованием биологически реалистичных механизмов.

В данной статье мы подробно разберём теоретические основы R-STDP и их конкретную реализацию в симуляции "существа в одномерном мире".

---

## Теоретические основы

### Классический STDP

**Spike-Timing Dependent Plasticity (STDP)** — это форма синаптической пластичности, при которой изменение синаптического веса зависит от относительного времени спайков пре- и постсинаптического нейронов.

Классическое правило STDP:

$$
\Delta w_{ij} = \begin{cases}
A_+ \exp\left(-\frac{\Delta t}{\tau_+}\right), & \text{если } \Delta t > 0 \text{ (пре до пост)} \\
-A_- \exp\left(\frac{\Delta t}{\tau_-}\right), & \text{если } \Delta t < 0 \text{ (пост до пре)}
\end{cases}
$$

где:
- $\Delta t = t_{\text{post}} - t_{\text{pre}}$ — разница времён спайков
- $A_+, A_-$ — амплитуды потенциации и депрессии
- $\tau_+, \tau_-$ — временные константы

**Интерпретация:**
- Если пресинаптический нейрон активируется *до* постсинаптического → связь **усиливается** (LTP)
- Если пресинаптический нейрон активируется *после* постсинаптического → связь **ослабевает** (LTD)

### Проблема отложенного вознаграждения

Классический STDP имеет фундаментальную проблему для задач обучения с подкреплением:

> **Проблема присвоения заслуг (Credit Assignment Problem):**
> Reward-сигнал обычно приходит с задержкой относительно действий, которые к нему привели. Как определить, какие именно синапсы "заслуживают" модификации?

Например, в нашей симуляции:
1. Существо видит еду слева (сенсорный вход)
2. Сеть генерирует команду "двигаться влево" (действие)
3. Существо перемещается
4. Только *после* перемещения среда генерирует reward

К моменту получения reward корреляции спайков уже "забыты". STDP не знает, какие синапсы усилить.

### R-STDP: Трёхфакторное правило обучения

**R-STDP** решает эту проблему введением **третьего фактора** — глобального сигнала подкрепления $r(t)$:

$$
\Delta w_{ij} = \eta \cdot r(t) \cdot e_{ij}(t)
$$

где:
- $\eta$ — скорость обучения (learning rate)
- $r(t)$ — reward-сигнал в момент времени $t$
- $e_{ij}(t)$ — **eligibility trace** (след приемлемости) синапса $w_{ij}$

Это **трёхфакторное правило**:
1. **Пресинаптическая активность** (входит в eligibility trace)
2. **Постсинаптическая активность** (входит в eligibility trace)
3. **Reward-сигнал** (модулирует изменение)

### Eligibility Traces (следы приемлемости)

**Eligibility trace** — это временная "метка" на синапсе, которая:
- Накапливается при совпадении пре- и постсинаптической активности
- Экспоненциально затухает со временем
- "Помнит" недавние корреляции до прихода reward

Динамика eligibility trace:

$$
\frac{de_{ij}}{dt} = -\frac{e_{ij}}{\tau_e} + \text{STDP}_{ij}(t)
$$

или в дискретной форме:

$$
e_{ij}(t+1) = \lambda \cdot e_{ij}(t) + \text{post}_i(t) \cdot \text{pre}_j(t)
$$

где:
- $\lambda = e^{-\Delta t / \tau_e}$ — коэффициент затухания (trace decay)
- $\tau_e$ — временная константа следа
- $\text{post}_i(t)$ — активация постсинаптического нейрона $i$
- $\text{pre}_j(t)$ — активация пресинаптического нейрона $j$

**Ключевая идея:** Eligibility trace "запоминает" хеббовские корреляции на время, достаточное для прихода отложенного reward. Когда reward приходит, он модулирует *все* синапсы пропорционально их eligibility.

---

## Реализация в симуляции

### Архитектура нейронной сети

В нашей симуляции используется многослойная нейронная сеть:

```
Архитектура: 4 → 6 → 4 → 2

Слой 0: 4 сенсорных нейрона → 6 скрытых (24 синапса)
Слой 1: 6 скрытых → 4 скрытых (24 синапса)  
Слой 2: 4 скрытых → 2 моторных (8 синапсов)

Всего: 56 пластичных синапсов
```

**Сенсорные нейроны (вход):**
| Индекс | Сигнал |
|--------|--------|
| 0 | `food_left` — еда слева |
| 1 | `food_right` — еда справа |
| 2 | `danger_left` — опасность слева |
| 3 | `danger_right` — опасность справа |

**Моторные нейроны (выход):**
| Индекс | Действие |
|--------|----------|
| 0 | `LEFT` — движение влево |
| 1 | `RIGHT` — движение вправо |

Выбор действия: **winner-take-all** — выбирается нейрон с максимальной активацией.

### Генерация Reward-сигнала

Reward-сигнал $r$ генерируется средой (`World`) на основе действия существа и текущей ситуации.

**Таблица reward-сигналов:**

| Ситуация | Действие | Reward $r$ | Обоснование |
|----------|----------|------------|-------------|
| Еда в мире | Движение к еде | $+1.0$ | Правильное поведение |
| Еда в мире | Движение от еды | $-0.5$ | Неоптимальное поведение |
| Опасность в мире | Движение от опасности | $+1.0$ | Правильное поведение |
| Опасность в мире | Движение к опасности | $-1.0$ | Опасное поведение (сильный штраф) |
| Еда рядом (dist < 1) | STAY | $+1.0$ | Достижение цели |
| Любая другая | STAY | $-0.3$ | Пассивность штрафуется |
| Нет сущности | Любое | $0.0$ | Нет обратной связи |

**Код генерации reward (`world.py`):**

```python
def _calculate_reward(self, old_pos: float, action: Action) -> float:
    if self.entity_pos is None:
        return 0.0
    
    distance = abs(old_pos - self.entity_pos)
    
    if action == Action.STAY:
        if self.entity_type == EntityType.FOOD and distance < 1.0:
            return 1.0   # Достиг еды — награда
        else:
            return -0.3  # Пассивность — штраф
    
    moved_left = (action == Action.LEFT)
    moved_right = (action == Action.RIGHT)
    entity_is_left = (self.entity_pos < old_pos)
    entity_is_right = (self.entity_pos > old_pos)
    
    if self.entity_type == EntityType.FOOD:
        if (moved_left and entity_is_left) or (moved_right and entity_is_right):
            return 1.0   # К еде
        else:
            return -0.5  # От еды
    
    elif self.entity_type == EntityType.DANGER:
        if (moved_left and entity_is_right) or (moved_right and entity_is_left):
            return 1.0   # От опасности
        else:
            return -1.0  # К опасности (сильный штраф!)
    
    return 0.0
```

**Важные свойства reward-сигнала:**

1. **Скалярность:** $r \in \{-1.0, -0.5, -0.3, 0.0, +1.0\}$ — одно число для всей сети
2. **Глобальность:** Один и тот же $r$ применяется ко *всем* слоям и синапсам
3. **Знак определяет направление:** $r > 0$ усиливает корреляции, $r < 0$ ослабляет
4. **Асимметрия штрафов:** Движение к опасности ($-1.0$) штрафуется сильнее, чем от еды ($-0.5$)

### Прямой проход и накопление Eligibility Traces

При каждом шаге симуляции выполняется **прямой проход** через сеть с одновременным обновлением eligibility traces.

**Алгоритм прямого прохода (`creature.py`):**

```python
def forward(self, sensory_input: Tuple[int, int, int, int]) -> np.ndarray:
    x = np.array(sensory_input, dtype=np.float32)
    
    for layer in self.layers:
        # Сохраняем вход для обучения
        layer.input_cache = x.copy()
        
        # Прямой проход: y = ReLU(W @ x)
        pre_activation = layer.weights @ x
        layer.activation = self._relu(pre_activation)
        
        # Обновление eligibility trace
        layer.eligibility = (
            self.trace_decay * layer.eligibility +
            np.outer(layer.activation, x)
        )
        
        x = layer.activation
    
    return x  # Активации моторных нейронов
```

**Математически для каждого слоя $l$:**

1. **Линейная комбинация:**
$$
z_i^{(l)} = \sum_j w_{ij}^{(l)} \cdot x_j^{(l-1)}
$$

2. **Активация (ReLU):**
$$
a_i^{(l)} = \max(0, z_i^{(l)})
$$

3. **Обновление eligibility trace:**
$$
e_{ij}^{(l)}(t) = \lambda \cdot e_{ij}^{(l)}(t-1) + a_i^{(l)}(t) \cdot x_j^{(l-1)}(t)
$$

где $\lambda = 0.7$ (параметр `trace_decay`).

**Интерпретация eligibility trace:**

Элемент $e_{ij}$ матрицы eligibility представляет собой **экспоненциально взвешенную сумму** произведений активаций:

$$
e_{ij}(t) = \sum_{k=0}^{t} \lambda^{t-k} \cdot a_i(k) \cdot x_j(k)
$$

Это "память" о том, насколько часто и сильно пресинаптический нейрон $j$ активировался одновременно с постсинаптическим нейроном $i$ за последние несколько шагов.

### Модуляция синаптических весов

Когда приходит reward-сигнал, он модулирует все синапсы пропорционально их eligibility traces.

**Алгоритм обучения (`creature.py`):**

```python
def learn(self, sensory_input, action, reward):
    # 1. Weight decay (забывание)
    for layer in self.layers:
        layer.weights *= (1.0 - self.weight_decay)
    
    # Если reward = 0, только decay
    if reward == 0 or action == Action.STAY:
        return
    
    # 2. Определяем активный моторный нейрон
    active_motor = 0 if action == Action.LEFT else 1
    
    # 3. Обновляем веса каждого слоя
    num_layers = len(self.layers)
    
    for i, layer in enumerate(self.layers):
        # Learning rate зависит от глубины слоя
        layer_lr = self.learning_rate * (0.5 + 0.5 * (i + 1) / num_layers)
        
        # Клиппинг eligibility для стабильности
        layer.eligibility = np.clip(layer.eligibility, -5.0, 5.0)
        
        # R-STDP: ΔW = η * r * e
        delta_w = layer_lr * reward * layer.eligibility
        
        # Competitive learning для выходного слоя
        if i == num_layers - 1:
            active_mask = np.zeros(self.num_motor)
            active_mask[active_motor] = 2.0       # Усиление активного
            active_mask[1 - active_motor] = -0.5  # Подавление неактивного
            delta_w = delta_w * active_mask[:, np.newaxis]
        
        # Применяем обновление
        layer.weights += delta_w
        
        # Ограничиваем веса
        layer.weights = np.clip(layer.weights, 0.0, 3.0)
        
        # Частичный сброс eligibility после reward
        layer.eligibility *= 0.3
```

**Ключевое уравнение R-STDP:**

$$
\Delta w_{ij}^{(l)} = \eta^{(l)} \cdot r \cdot e_{ij}^{(l)}
$$

где:
- $\eta^{(l)} = \eta_{\text{base}} \cdot \left(0.5 + 0.5 \cdot \frac{l+1}{L}\right)$ — learning rate слоя $l$
- $r$ — reward-сигнал
- $e_{ij}^{(l)}$ — eligibility trace синапса

### Дополнительные механизмы

#### 1. Weight Decay (забывание)

На каждом шаге все веса экспоненциально затухают:

$$
w_{ij}(t+1) = w_{ij}(t) \cdot (1 - \delta)
$$

где $\delta = 0.001$ — коэффициент затухания.

**Назначение:** Без подкрепления веса постепенно "забываются", что:
- Предотвращает накопление шума
- Позволяет переобучаться при изменении задачи
- Моделирует биологический распад синапсов

#### 2. Competitive Learning (конкурентное обучение)

Для выходного слоя применяется **winner-take-all** модуляция:

$$
\Delta w_{ij}^{\text{output}} = \Delta w_{ij} \cdot m_i
$$

где маска $m_i$:
$$
m_i = \begin{cases}
+2.0, & \text{если нейрон } i \text{ победил (активен)} \\
-0.5, & \text{если нейрон } i \text{ проиграл (неактивен)}
\end{cases}
$$

**Эффект:**
- При $r > 0$: активный нейрон усиливается, неактивный ослабевает
- При $r < 0$: активный нейрон ослабевает, неактивный усиливается

Это создаёт **антагонистическую динамику** между моторными нейронами.

#### 3. Eligibility Reset после Reward

После применения reward eligibility traces частично сбрасываются:

$$
e_{ij}(t^+) = 0.3 \cdot e_{ij}(t^-)
$$

**Назначение:** Предотвращает "двойной счёт" — одна и та же корреляция не должна многократно усиливаться.

#### 4. Epsilon-Greedy Exploration

Для исследования пространства действий используется $\varepsilon$-greedy стратегия с annealing:

$$
\varepsilon(t+1) = \max(\varepsilon_{\min}, \varepsilon(t) \cdot \gamma)
$$

где:
- $\varepsilon_{\text{start}} = 0.3$ (30% случайных действий в начале)
- $\varepsilon_{\min} = 0.02$ (2% в конце)
- $\gamma = 0.995$ — коэффициент затухания

---

## Математическое описание

### Полная система уравнений

Обозначения:
- $L$ — число слоёв
- $n_l$ — число нейронов в слое $l$
- $W^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}$ — матрица весов слоя $l$
- $E^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}$ — матрица eligibility traces слоя $l$
- $\mathbf{a}^{(l)} \in \mathbb{R}^{n_l}$ — вектор активаций слоя $l$
- $\mathbf{x} = \mathbf{a}^{(0)}$ — входной вектор (сенсоры)
- $r \in \mathbb{R}$ — reward-сигнал

**1. Прямой проход (для каждого слоя $l = 1, \ldots, L$):**

$$
\mathbf{z}^{(l)} = W^{(l)} \mathbf{a}^{(l-1)}
$$

$$
\mathbf{a}^{(l)} = \text{ReLU}(\mathbf{z}^{(l)}) = \max(\mathbf{0}, \mathbf{z}^{(l)})
$$

**2. Обновление eligibility traces:**

$$
E^{(l)}(t) = \lambda \cdot E^{(l)}(t-1) + \mathbf{a}^{(l)}(t) \otimes \mathbf{a}^{(l-1)}(t)
$$

где $\otimes$ — внешнее произведение, $\lambda \in (0, 1)$ — trace decay.

**3. Выбор действия:**

$$
a^* = \begin{cases}
\text{random}(\{\text{LEFT}, \text{RIGHT}\}), & \text{с вероятностью } \varepsilon \\
\arg\max_i a_i^{(L)}, & \text{с вероятностью } 1 - \varepsilon
\end{cases}
$$

**4. Вычисление reward:**

$$
r = R(\text{state}, a^*)
$$

где $R$ — функция вознаграждения среды.

**5. Обновление весов (R-STDP):**

$$
W^{(l)}(t+1) = (1 - \delta) \cdot W^{(l)}(t) + \eta^{(l)} \cdot r \cdot M^{(l)} \odot E^{(l)}(t)
$$

где:
- $\delta$ — weight decay
- $\eta^{(l)}$ — learning rate слоя
- $M^{(l)}$ — маска competitive learning (только для $l = L$)
- $\odot$ — поэлементное умножение

**6. Сброс eligibility после reward:**

$$
E^{(l)}(t^+) = \alpha \cdot E^{(l)}(t^-)
$$

где $\alpha = 0.3$.

**7. Annealing exploration:**

$$
\varepsilon(t+1) = \max(\varepsilon_{\min}, \gamma \cdot \varepsilon(t))
$$

### Пример: один цикл обучения

Рассмотрим конкретный пример для понимания.

**Ситуация:** Еда слева, существо в центре.

**Шаг 1: Сенсорный вход**
$$
\mathbf{x} = (1, 0, 0, 0)^T \quad \text{(food\_left = 1)}
$$

**Шаг 2: Прямой проход**

Пусть после прохода через сеть:
$$
\mathbf{a}^{(3)} = (0.8, 0.3)^T \quad \text{(LEFT > RIGHT)}
$$

**Шаг 3: Выбор действия**

Если $\varepsilon < \text{random}()$, выбираем $a^* = \text{LEFT}$ (индекс 0).

**Шаг 4: Получение reward**

Существо двинулось влево к еде:
$$
r = +1.0
$$

**Шаг 5: Обновление весов**

Для выходного слоя с маской competitive learning:

$$
\Delta W^{(3)} = \eta \cdot (+1.0) \cdot \begin{pmatrix} +2.0 \\ -0.5 \end{pmatrix} \odot E^{(3)}
$$

Это означает:
- Веса к нейрону LEFT **усиливаются** (reward × 2.0 × eligibility)
- Веса к нейрону RIGHT **ослабевают** (reward × (-0.5) × eligibility)

**Результат:** Сеть "запоминает", что при входе $(1,0,0,0)$ нужно активировать LEFT.

---

## Заключение

### Ключевые идеи R-STDP

1. **Eligibility traces** решают проблему отложенного вознаграждения, сохраняя "память" о недавних корреляциях активности.

2. **Reward-сигнал** выступает глобальным модулятором, определяющим *направление* и *величину* изменения весов.

3. **Трёхфакторное правило** $\Delta w = \eta \cdot r \cdot e$ объединяет локальную хеббовскую пластичность с глобальным сигналом подкрепления.

4. **Weight decay** обеспечивает "забывание" и предотвращает неограниченный рост весов.

5. **Competitive learning** на выходном слое создаёт чёткое разделение между альтернативными действиями.

### Биологическая правдоподобность

R-STDP имеет экспериментальное подтверждение в нейробиологии:
- Дофамин модулирует синаптическую пластичность в базальных ганглиях
- Eligibility traces соответствуют молекулярным каскадам в синапсах
- Временное окно eligibility (~секунды) соответствует времени действия вторичных мессенджеров

### Преимущества для нейроморфных систем

1. **Локальность вычислений** — каждый синапс обновляется на основе локальной информации + глобального скаляра
2. **Онлайн-обучение** — не требуется хранение истории или обратное распространение
3. **Энергоэффективность** — совместимо с событийно-управляемыми (event-driven) архитектурами
4. **Аппаратная реализуемость** — может быть реализовано в нейроморфных чипах (Intel Loihi, IBM TrueNorth)

---

*Документ создан для проекта симуляции R-STDP обучения существа в одномерном мире.*

