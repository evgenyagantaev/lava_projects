{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ANN vs SNN Equivalence Demo\n",
        "\n",
        "This notebook demonstrates that a Spiking Neural Network (SNN) using Sigma-Delta neurons \n",
        "produces the same output as a traditional Artificial Neural Network (ANN) with ReLU activation,\n",
        "when both networks have identical weights.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Input (3) → [Dense W1] → Hidden (4, ReLU) → [Dense W2] → Output (2, ReLU)\n",
        "```\n",
        "\n",
        "## Key Insight\n",
        "\n",
        "Sigma-Delta neurons work by:\n",
        "1. **Delta encoder**: Encodes changes in input as sparse spikes\n",
        "2. **Sigma decoder**: Accumulates spikes to reconstruct continuous values\n",
        "3. **Activation**: Applies ReLU (same as ANN)\n",
        "\n",
        "For constant input, Delta sends the value once, then zeros. Sigma holds it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lava imports\n",
        "from lava.proc.io.source import RingBuffer as InputBuffer\n",
        "from lava.proc.dense.process import Dense\n",
        "from lava.proc.sdn.process import Sigma, Delta, SigmaDelta, ActivationMode\n",
        "from lava.proc.monitor.process import Monitor\n",
        "from lava.magma.core.run_conditions import RunSteps\n",
        "from lava.magma.core.run_configs import Loihi2SimCfg\n",
        "\n",
        "np.random.seed(42)  # For reproducibility\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Network Architecture and Random Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Network dimensions\n",
        "INPUT_SIZE = 3\n",
        "HIDDEN_SIZE = 4\n",
        "OUTPUT_SIZE = 2\n",
        "\n",
        "# Generate random weights (same for both ANN and SNN)\n",
        "W1 = np.random.randn(HIDDEN_SIZE, INPUT_SIZE) * 0.5  # Shape: (4, 3)\n",
        "W2 = np.random.randn(OUTPUT_SIZE, HIDDEN_SIZE) * 0.5  # Shape: (2, 4)\n",
        "\n",
        "print(f\"W1 shape: {W1.shape}\")\n",
        "print(f\"W1:\\n{W1}\\n\")\n",
        "print(f\"W2 shape: {W2.shape}\")\n",
        "print(f\"W2:\\n{W2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Traditional ANN (NumPy)\n",
        "\n",
        "A simple feedforward network with ReLU activations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    \"\"\"ReLU activation function\"\"\"\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def ann_forward(x, w1, w2):\n",
        "    \"\"\"\n",
        "    Forward pass through 2-layer ANN.\n",
        "    \n",
        "    Args:\n",
        "        x: Input vector (INPUT_SIZE,)\n",
        "        w1: First layer weights (HIDDEN_SIZE, INPUT_SIZE)\n",
        "        w2: Second layer weights (OUTPUT_SIZE, HIDDEN_SIZE)\n",
        "    \n",
        "    Returns:\n",
        "        Output vector (OUTPUT_SIZE,)\n",
        "    \"\"\"\n",
        "    # Layer 1: Input -> Hidden\n",
        "    h = relu(w1 @ x)\n",
        "    \n",
        "    # Layer 2: Hidden -> Output\n",
        "    y = relu(w2 @ h)\n",
        "    \n",
        "    return y, h  # Return hidden activations too for debugging\n",
        "\n",
        "\n",
        "# Test input - positive values work better for ReLU demo\n",
        "x_input = np.array([1.0, 0.8, 0.5])\n",
        "\n",
        "# Run ANN\n",
        "ann_output, ann_hidden = ann_forward(x_input, W1, W2)\n",
        "\n",
        "print(f\"Input: {x_input}\")\n",
        "print(f\"Hidden layer (after ReLU): {ann_hidden}\")\n",
        "print(f\"ANN Output: {ann_output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Spiking Neural Network (Lava with Sigma-Delta neurons)\n",
        "\n",
        "### Architecture for SNN:\n",
        "```\n",
        "Input → Delta(encode) → Dense(W1) → SigmaDelta(hidden,ReLU) → Dense(W2) → SigmaDelta(output,ReLU)\n",
        "```\n",
        "\n",
        "The Delta encoder at the input:\n",
        "- At t=0: sends the input value (change from 0)\n",
        "- At t>0: sends 0 (no change in constant input)\n",
        "\n",
        "This allows Sigma in SigmaDelta to accumulate once and hold the value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SNN Parameters\n",
        "T_SIM = 30       # Number of timesteps to run\n",
        "VTH = 0.01       # Sigma-Delta threshold (smaller = more accurate)\n",
        "\n",
        "# Prepare input data: constant input repeated over time\n",
        "# Shape: (INPUT_SIZE, T_SIM)\n",
        "inp_data = np.tile(x_input.reshape(-1, 1), (1, T_SIM))\n",
        "\n",
        "print(f\"Input data shape: {inp_data.shape}\")\n",
        "print(f\"Input is constant: {x_input} repeated for {T_SIM} timesteps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build SNN\n",
        "# ---------\n",
        "\n",
        "# Input source (sends input values)\n",
        "source = InputBuffer(data=inp_data)\n",
        "\n",
        "# Identity projection from source to delta encoder\n",
        "inp_proj = Dense(weights=np.eye(INPUT_SIZE), num_message_bits=24)\n",
        "\n",
        "# Delta encoder for input - encodes changes only\n",
        "inp_delta = Delta(\n",
        "    shape=(INPUT_SIZE,),\n",
        "    vth=VTH,\n",
        "    cum_error=True\n",
        ")\n",
        "\n",
        "# Layer 1: Input -> Hidden\n",
        "dense1 = Dense(weights=W1, num_message_bits=24)\n",
        "\n",
        "# Hidden layer neurons (Sigma-Delta with ReLU)\n",
        "hidden_neurons = SigmaDelta(\n",
        "    shape=(HIDDEN_SIZE,),\n",
        "    vth=VTH,\n",
        "    act_mode=ActivationMode.RELU,\n",
        "    cum_error=True\n",
        ")\n",
        "\n",
        "# Layer 2: Hidden -> Output\n",
        "dense2 = Dense(weights=W2, num_message_bits=24)\n",
        "\n",
        "# Output layer neurons (Sigma-Delta with ReLU)\n",
        "output_neurons = SigmaDelta(\n",
        "    shape=(OUTPUT_SIZE,),\n",
        "    vth=VTH,\n",
        "    act_mode=ActivationMode.RELU,\n",
        "    cum_error=True\n",
        ")\n",
        "\n",
        "# Connect the network\n",
        "# Input -> Delta encoder\n",
        "source.s_out.connect(inp_proj.s_in)\n",
        "inp_proj.a_out.connect(inp_delta.a_in)\n",
        "\n",
        "# Delta encoder -> Layer 1\n",
        "inp_delta.s_out.connect(dense1.s_in)\n",
        "dense1.a_out.connect(hidden_neurons.a_in)\n",
        "\n",
        "# Layer 1 -> Layer 2\n",
        "hidden_neurons.s_out.connect(dense2.s_in)\n",
        "dense2.a_out.connect(output_neurons.a_in)\n",
        "\n",
        "# Monitor to record activations over time\n",
        "monitor_out = Monitor()\n",
        "monitor_out.probe(output_neurons.act, T_SIM)\n",
        "\n",
        "monitor_hidden = Monitor()\n",
        "monitor_hidden.probe(hidden_neurons.act, T_SIM)\n",
        "\n",
        "# Also monitor sigma (accumulated input)\n",
        "monitor_sigma_hidden = Monitor()\n",
        "monitor_sigma_hidden.probe(hidden_neurons.sigma, T_SIM)\n",
        "\n",
        "print(\"SNN built successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run SNN simulation\n",
        "output_neurons.run(\n",
        "    condition=RunSteps(num_steps=T_SIM),\n",
        "    run_cfg=Loihi2SimCfg(select_tag='floating_pt')\n",
        ")\n",
        "\n",
        "# Get recorded data\n",
        "out_data = monitor_out.get_data()\n",
        "hidden_data = monitor_hidden.get_data()\n",
        "sigma_data = monitor_sigma_hidden.get_data()\n",
        "\n",
        "# Stop the runtime\n",
        "output_neurons.stop()\n",
        "\n",
        "# Extract activation traces\n",
        "out_proc_key = list(out_data.keys())[0]\n",
        "hidden_proc_key = list(hidden_data.keys())[0]\n",
        "sigma_proc_key = list(sigma_data.keys())[0]\n",
        "\n",
        "snn_output_trace = out_data[out_proc_key]['act']\n",
        "snn_hidden_trace = hidden_data[hidden_proc_key]['act']\n",
        "snn_sigma_trace = sigma_data[sigma_proc_key]['sigma']\n",
        "\n",
        "print(f\"SNN output trace shape: {snn_output_trace.shape}\")\n",
        "print(f\"SNN hidden trace shape: {snn_hidden_trace.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare ANN and SNN outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualization\n",
        "\n",
        "Let's see how the SNN activations converge to the ANN values over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "time_steps = np.arange(T_SIM)\n",
        "\n",
        "# Plot Hidden layer activations\n",
        "ax = axes[0, 0]\n",
        "for i in range(HIDDEN_SIZE):\n",
        "    ax.plot(time_steps, snn_hidden_trace[:, i], label=f'SNN Hidden[{i}]', linewidth=2)\n",
        "    ax.axhline(y=ann_hidden[i], color=f'C{i}', linestyle='--', alpha=0.7)\n",
        "ax.set_xlabel('Time step')\n",
        "ax.set_ylabel('Activation')\n",
        "ax.set_title('Hidden Layer: SNN (solid) vs ANN (dashed)')\n",
        "ax.legend(loc='upper right', fontsize=8)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Output layer activations\n",
        "ax = axes[0, 1]\n",
        "for i in range(OUTPUT_SIZE):\n",
        "    ax.plot(time_steps, snn_output_trace[:, i], label=f'SNN Output[{i}]', linewidth=2)\n",
        "    ax.axhline(y=ann_output[i], color=f'C{i}', linestyle='--', alpha=0.7,\n",
        "               label=f'ANN Output[{i}] = {ann_output[i]:.4f}')\n",
        "ax.set_xlabel('Time step')\n",
        "ax.set_ylabel('Activation')\n",
        "ax.set_title('Output Layer: SNN (solid) vs ANN (dashed)')\n",
        "ax.legend(loc='upper right', fontsize=8)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot Sigma (accumulated input) for hidden layer\n",
        "ax = axes[1, 0]\n",
        "for i in range(HIDDEN_SIZE):\n",
        "    ax.plot(time_steps, snn_sigma_trace[:, i], label=f'Sigma[{i}]', linewidth=2)\n",
        "    # Expected sigma = W1 @ x_input (before ReLU)\n",
        "    expected_sigma = (W1 @ x_input)[i]\n",
        "    ax.axhline(y=expected_sigma, color=f'C{i}', linestyle='--', alpha=0.7)\n",
        "ax.set_xlabel('Time step')\n",
        "ax.set_ylabel('Sigma (accumulated input)')\n",
        "ax.set_title('Hidden Layer Sigma: SNN (solid) vs Expected W1@x (dashed)')\n",
        "ax.legend(loc='upper right', fontsize=8)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Bar chart comparison\n",
        "ax = axes[1, 1]\n",
        "x_pos = np.arange(HIDDEN_SIZE)\n",
        "width = 0.35\n",
        "\n",
        "# Hidden layer bars\n",
        "ax.bar(x_pos - width/2, ann_hidden, width, label='ANN Hidden', alpha=0.8)\n",
        "ax.bar(x_pos + width/2, snn_hidden, width, label='SNN Hidden', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Neuron index')\n",
        "ax.set_ylabel('Activation')\n",
        "ax.set_title('Final Activations: ANN vs SNN')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ann_snn_comparison.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPlot saved to: ann_snn_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "This demonstration shows that:\n",
        "\n",
        "1. **SNN with Sigma-Delta neurons is functionally equivalent to ANN with ReLU**\n",
        "2. **Same weights produce same outputs** (within quantization tolerance defined by `vth`)\n",
        "3. **Delta encoding** is key - it encodes only changes, allowing Sigma to hold values\n",
        "\n",
        "### Key Architecture for Equivalence\n",
        "\n",
        "```\n",
        "ANN:  Input → Dense(W) → ReLU → Output\n",
        "SNN:  Input → Delta → Dense(W) → SigmaDelta(ReLU) → Output\n",
        "```\n",
        "\n",
        "### Practical Implications\n",
        "\n",
        "- Take any pre-trained ANN weights\n",
        "- Load them into an isomorphic SNN (using Sigma-Delta neurons)\n",
        "- Get the same computational results\n",
        "- Benefit from energy efficiency of sparse spike-based computation on Loihi2\n",
        "\n",
        "### Parameters that affect accuracy\n",
        "\n",
        "- `vth` (threshold): Lower = more accurate but more spikes\n",
        "- `cum_error`: Cumulative error helps with precision\n",
        "- `num_message_bits`: Higher = more precision in graded spikes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get final SNN output (at last timestep, when it has converged)\n",
        "snn_output = snn_output_trace[-1]\n",
        "snn_hidden = snn_hidden_trace[-1]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARISON: ANN vs SNN\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nInput: {x_input}\")\n",
        "\n",
        "print(f\"\\n--- Hidden Layer ---\")\n",
        "print(f\"ANN hidden: {ann_hidden}\")\n",
        "print(f\"SNN hidden: {snn_hidden}\")\n",
        "print(f\"Difference: {np.abs(ann_hidden - snn_hidden)}\")\n",
        "print(f\"Max error:  {np.max(np.abs(ann_hidden - snn_hidden)):.6f}\")\n",
        "\n",
        "print(f\"\\n--- Output Layer ---\")\n",
        "print(f\"ANN output: {ann_output}\")\n",
        "print(f\"SNN output: {snn_output}\")\n",
        "print(f\"Difference: {np.abs(ann_output - snn_output)}\")\n",
        "print(f\"Max error:  {np.max(np.abs(ann_output - snn_output)):.6f}\")\n",
        "\n",
        "# Check if outputs match within tolerance\n",
        "tolerance = VTH * 3  # Allow some tolerance based on threshold\n",
        "hidden_match = np.allclose(ann_hidden, snn_hidden, atol=tolerance)\n",
        "output_match = np.allclose(ann_output, snn_output, atol=tolerance)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if hidden_match and output_match:\n",
        "    print(f\"SUCCESS: ANN and SNN outputs match within tolerance ({tolerance:.4f})!\")\n",
        "else:\n",
        "    print(f\"Hidden match: {hidden_match}, Output match: {output_match}\")\n",
        "    print(f\"Tolerance used: {tolerance:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
