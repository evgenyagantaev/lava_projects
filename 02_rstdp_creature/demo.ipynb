{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# R-STDP: Creature Learning in 1D World\n",
        "\n",
        "This notebook demonstrates **Reward-modulated Spike-Timing Dependent Plasticity (R-STDP)** - a biologically plausible learning rule that combines Hebbian learning with reinforcement signals.\n",
        "\n",
        "## The Problem\n",
        "\n",
        "A simple creature lives in a 1D world. It must learn to:\n",
        "- **Approach food** (green) â†’ positive reward\n",
        "- **Avoid danger** (red) â†’ positive reward for moving away\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                   SENSORY NEURONS (4)                   â”‚\n",
        "â”‚   food_left   food_right   danger_left   danger_right   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚        â”‚            â”‚            â”‚\n",
        "           â–¼        â–¼            â–¼            â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚              Plastic Synapses (R-STDP)                  â”‚\n",
        "â”‚         Weights change based on reward signal           â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â–¼                                  â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚    MOTOR: LEFT      â”‚          â”‚    MOTOR: RIGHT     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import our modules\n",
        "from world import World, Action, EntityType\n",
        "from creature import CreatureBrain\n",
        "from simulation import Simulation, EpisodeStats\n",
        "from visualization import (\n",
        "    plot_training_progress,\n",
        "    plot_weight_evolution,\n",
        "    plot_weight_matrix,\n",
        "    plot_world_state,\n",
        "    plot_behavior_comparison\n",
        ")\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create the World\n",
        "\n",
        "Our 1D world spans from -10 to +10. Food and danger appear randomly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the world\n",
        "world = World(\n",
        "    world_size=10.0,      # World spans [-10, 10]\n",
        "    spawn_prob=0.4,       # 40% chance to spawn entity each step\n",
        "    despawn_prob=0.1,     # 10% chance to despawn\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Reset and show initial state\n",
        "state = world.reset()\n",
        "print(\"Initial world state:\")\n",
        "print(world.render_ascii(width=50))\n",
        "print(f\"\\nSensory input: {world.get_sensory_input()}\")\n",
        "print(\"  (food_left, food_right, danger_left, danger_right)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create the Creature's Brain\n",
        "\n",
        "The brain has plastic synapses that will be trained with R-STDP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the brain with random initial weights\n",
        "brain = CreatureBrain(\n",
        "    learning_rate=0.3,    # How fast to learn\n",
        "    initial_weight=0.5,   # Initial random weight range\n",
        "    seed=123\n",
        ")\n",
        "\n",
        "print(\"Initial synaptic weights:\")\n",
        "print(brain.get_weights())\n",
        "print(\"\\nWeight matrix layout:\")\n",
        "print(\"         [food_L, food_R, danger_L, danger_R]\")\n",
        "print(\"move_L   [  w00,    w01,     w02,      w03  ]\")\n",
        "print(\"move_R   [  w10,    w11,     w12,      w13  ]\")\n",
        "\n",
        "# Visualize initial weights\n",
        "fig = plot_weight_matrix(brain.get_weights(), \"Initial Weights (Random)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate BEFORE Training\n",
        "\n",
        "Let's see how the creature behaves with random weights (no training yet).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simulation\n",
        "sim = Simulation(world, brain)\n",
        "\n",
        "# Run evaluation WITHOUT learning\n",
        "print(\"=\"*50)\n",
        "print(\"BEFORE TRAINING - Random behavior\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "pre_training_stats = []\n",
        "for ep in range(5):\n",
        "    stats = sim.run_episode(num_steps=50, learn=False)\n",
        "    pre_training_stats.append(stats)\n",
        "    print(f\"Episode {ep+1}: Reward={stats.total_reward:.0f}, Accuracy={stats.accuracy:.1%}\")\n",
        "\n",
        "avg_pre_accuracy = np.mean([s.accuracy for s in pre_training_stats])\n",
        "print(f\"\\nAverage accuracy: {avg_pre_accuracy:.1%} (random ~ 50%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train with R-STDP\n",
        "\n",
        "Now let's train the creature. The R-STDP rule will:\n",
        "- **Strengthen** connections that led to rewarded actions\n",
        "- **Weaken** connections that led to unrewarded actions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset brain for training\n",
        "brain.reset_weights()\n",
        "\n",
        "# Training parameters\n",
        "NUM_EPISODES = 30\n",
        "STEPS_PER_EPISODE = 100\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"TRAINING with R-STDP\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Episodes: {NUM_EPISODES}, Steps per episode: {STEPS_PER_EPISODE}\\n\")\n",
        "\n",
        "training_stats = []\n",
        "for ep in range(NUM_EPISODES):\n",
        "    stats = sim.run_episode(num_steps=STEPS_PER_EPISODE, learn=True)\n",
        "    training_stats.append(stats)\n",
        "    \n",
        "    if (ep + 1) % 5 == 0:\n",
        "        recent_acc = np.mean([s.accuracy for s in training_stats[-5:]])\n",
        "        print(f\"Episode {ep+1:2d}: Reward={stats.total_reward:5.0f}, \"\n",
        "              f\"Accuracy={stats.accuracy:.1%}, Recent avg={recent_acc:.1%}\")\n",
        "\n",
        "print(f\"\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Training Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training progress\n",
        "fig = plot_training_progress(training_stats, \"R-STDP Training Progress\")\n",
        "plt.savefig('training_progress.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Analyze Learned Weights\n",
        "\n",
        "Let's see how the weights evolved during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot weight evolution\n",
        "fig = plot_weight_evolution(brain.weight_history)\n",
        "plt.savefig('weight_evolution.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Show final weights\n",
        "print(\"\\nFinal weights after training:\")\n",
        "print(brain.get_weights())\n",
        "\n",
        "fig = plot_weight_matrix(brain.get_weights(), \"Final Weights (After Training)\")\n",
        "plt.savefig('final_weights.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate AFTER Training\n",
        "\n",
        "Let's see how the trained creature performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate trained creature (no learning)\n",
        "print(\"=\"*50)\n",
        "print(\"AFTER TRAINING - Learned behavior\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "post_training_stats = []\n",
        "for ep in range(5):\n",
        "    stats = sim.run_episode(num_steps=50, learn=False)\n",
        "    post_training_stats.append(stats)\n",
        "    print(f\"Episode {ep+1}: Reward={stats.total_reward:.0f}, Accuracy={stats.accuracy:.1%}\")\n",
        "\n",
        "avg_post_accuracy = np.mean([s.accuracy for s in post_training_stats])\n",
        "print(f\"\\nAverage accuracy: {avg_post_accuracy:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Behavior Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze what the creature learned\n",
        "analysis = brain.get_expected_behavior()\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"BEHAVIOR ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nWhen FOOD is on the LEFT:   â†’ Creature moves {analysis['food_left_response']}\")\n",
        "print(f\"When FOOD is on the RIGHT:  â†’ Creature moves {analysis['food_right_response']}\")\n",
        "print(f\"When DANGER is on the LEFT: â†’ Creature moves {analysis['danger_left_response']}\")\n",
        "print(f\"When DANGER is on the RIGHT:â†’ Creature moves {analysis['danger_right_response']}\")\n",
        "\n",
        "print(f\"\\nâœ“ Food behavior correct:   {analysis['food_behavior_correct']}\")\n",
        "print(f\"âœ“ Danger behavior correct: {analysis['danger_behavior_correct']}\")\n",
        "print(f\"\\n{'ðŸŽ‰ FULLY TRAINED!' if analysis['fully_trained'] else 'âš ï¸ Needs more training'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare before and after training\n",
        "fig = plot_behavior_comparison(pre_training_stats, post_training_stats)\n",
        "plt.savefig('behavior_comparison.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Pre-training accuracy:  {avg_pre_accuracy:.1%}\")\n",
        "print(f\"Post-training accuracy: {avg_post_accuracy:.1%}\")\n",
        "print(f\"Improvement: {(avg_post_accuracy - avg_pre_accuracy)*100:+.1f}%\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Understanding R-STDP\n",
        "\n",
        "### How the learning works:\n",
        "\n",
        "1. **Sensory Input** â†’ Activates sensory neurons (food_left, food_right, danger_left, danger_right)\n",
        "\n",
        "2. **Motor Decision** â†’ Based on weighted sum: `motor_activation = weights @ sensory_input`\n",
        "\n",
        "3. **Action** â†’ Winner-take-all selects LEFT or RIGHT movement\n",
        "\n",
        "4. **Reward** â†’ Environment provides reward signal:\n",
        "   - +1 for moving toward food\n",
        "   - +1 for moving away from danger\n",
        "   - 0 otherwise\n",
        "\n",
        "5. **Weight Update** (R-STDP rule):\n",
        "   ```\n",
        "   Î”w = learning_rate Ã— reward Ã— pre_activity Ã— post_activity\n",
        "   ```\n",
        "   \n",
        "   - If reward > 0: strengthen connections that led to this action\n",
        "   - If reward < 0: weaken connections that led to this action\n",
        "\n",
        "### Expected final weights:\n",
        "\n",
        "| Weight | Should be | Why |\n",
        "|--------|-----------|-----|\n",
        "| food_left â†’ move_left | HIGH | Approach food on left |\n",
        "| food_right â†’ move_right | HIGH | Approach food on right |\n",
        "| danger_left â†’ move_right | HIGH | Escape danger on left |\n",
        "| danger_right â†’ move_left | HIGH | Escape danger on right |\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
